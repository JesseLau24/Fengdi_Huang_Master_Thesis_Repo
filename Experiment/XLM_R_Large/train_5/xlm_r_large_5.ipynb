{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e014f1",
   "metadata": {},
   "source": [
    "### Please download the model from Hugging Face\n",
    "\n",
    "### Hugging Face Model Repo:\n",
    "https://huggingface.co/JesseHuang922/lv_roberta_large\n",
    "\n",
    "Due to GitHub's file size limit (maximum 100MB per file), the larger RoBERTa-based models are not included in this repository.\n",
    "\n",
    "Specifically, files under ./models/* and ./packages/* are excluded from version control.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Cell 1ÔºöImport and directories\n",
    "# ==============================\n",
    "from pathlib import Path\n",
    "import os\n",
    "import spacy\n",
    "from spacy.lookups import Lookups\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.cli.package import package\n",
    "\n",
    "# Project root dir\n",
    "project_root = Path(\".\").resolve()\n",
    "\n",
    "# Project structure\n",
    "models_dir = project_root / \"models\"\n",
    "model_name = \"lv_roberta_large\"\n",
    "trained_model_path = models_dir / model_name / \"model-best\"\n",
    "final_model_path = models_dir / model_name / \"model_roberta_large\"\n",
    "lookups_path = project_root / \"lookups_lv\"\n",
    "package_output_dir = project_root / \"packages\"\n",
    "config_path = project_root / \"config\" / \"config_roberta_large_5.cfg\"\n",
    "\n",
    "# Create directories\n",
    "for p in [models_dir, models_dir / model_name, package_output_dir, lookups_path, project_root / \"config\"]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Imports and directories are created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06f6699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Cell 2ÔºöInitializing config\n",
    "# =============================\n",
    "!python -m spacy init config ./config/config_roberta_large_5.cfg \\\n",
    "    --lang lv \\\n",
    "    --pipeline transformer,tagger,morphologizer,parser,senter \\\n",
    "    --optimize efficiency \\\n",
    "    --gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c3863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Cell 3ÔºöModify config\n",
    "# ==========================\n",
    "\n",
    "# corpus dir\n",
    "subset = \"5pct\"  \n",
    "corpus_dir = Path(\"/home/jesse/Projects/myprojs/Master_Thesis/Fengdi_Huang_Master_Thesis_Repo/Corpus/train\")\n",
    "\n",
    "# Read config file\n",
    "cfg_text = config_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# Replace the training/validation set path\n",
    "cfg_text = cfg_text.replace(\"train = null\", f\"train = {corpus_dir}/lv_lvtb-ud-train-{subset}.spacy\")\n",
    "cfg_text = cfg_text.replace(\"dev = null\", f\"dev = {corpus_dir}/lv_lvtb-ud-dev.spacy\")\n",
    "\n",
    "# Change transformer model to xlm-roberta-large\n",
    "cfg_text = cfg_text.replace(\"bert-base-multilingual-uncased\", \"xlm-roberta-large\")\n",
    "\n",
    "# Turn on Mixed Precision\n",
    "cfg_text = cfg_text.replace(\"mixed_precision = false\", \"mixed_precision = true\")\n",
    "\n",
    "# Modify pipelineÔºö add trf_tok2vec component to pipeline\n",
    "cfg_text = cfg_text.replace(\n",
    "    'pipeline = [\"transformer\",\"tagger\",\"morphologizer\",\"parser\",\"senter\"]',\n",
    "    'pipeline = [\"transformer\",\"trf_tok2vec\",\"tagger\",\"morphologizer\",\"parser\",\"senter\"]'\n",
    ")\n",
    "\n",
    "# Add trf_tok2vec component config\n",
    "if \"[components.trf_tok2vec]\" not in cfg_text:\n",
    "    trf_tok2vec_cfg = \"\"\"\n",
    "[components.trf_tok2vec]\n",
    "factory = \"tok2vec\"\n",
    "\n",
    "[components.trf_tok2vec.model]\n",
    "@architectures = \"spacy-transformers.TransformerListener.v1\"\n",
    "grad_factor = 1.0\n",
    "pooling = {\"@layers\":\"reduce_mean.v1\"}\n",
    "upstream = \"*\"\n",
    "\"\"\"\n",
    "    cfg_text += trf_tok2vec_cfg\n",
    "\n",
    "config_path.write_text(cfg_text, encoding=\"utf-8\")\n",
    "print(\"Config updated: training/calidation set path, transformer base model, mixed precision and pipeline + trf_tok2vec components are all set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf4dc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Cell 4ÔºöGenerate lemma lookup table\n",
    "# ===================================\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.lookups import Lookups\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "corpus_dir= Path(\"/home/jesse/Projects/myprojs/Master_Thesis/Fengdi_Huang_Master_Thesis_Repo/Corpus/train\")\n",
    "lookups_path = Path(\"lookups_lv\")\n",
    "\n",
    "# --------------- Choose generation mode ---------------\n",
    "# Mode 1: Strict evaluation mode (train + dev only)\n",
    "# files = [\"lv_lvtb-ud-train.spacy\", \"lv_lvtb-ud-dev.spacy\"]\n",
    "\n",
    "# Mode 2: Practical enhanced mode (train + dev + test)\n",
    "files = [\"lv_lvtb-ud-train.spacy\", \"lv_lvtb-ud-dev.spacy\", \"lv_lvtb-ud-test.spacy\"] # more is always better, though here the return is mininal.\n",
    "\n",
    "# --------------- Generate lemma lookup ---------------\n",
    "lemma_dict = {}\n",
    "nlp_blank = spacy.blank(\"lv\")\n",
    "\n",
    "for file_name in files:\n",
    "    docbin = DocBin().from_disk(corpus_dir / file_name)\n",
    "    for doc in docbin.get_docs(nlp_blank.vocab):\n",
    "        for token in doc:\n",
    "            if token.lemma_:\n",
    "                lemma_dict[token.text.lower()] = token.lemma_\n",
    "\n",
    "lookups = Lookups()\n",
    "lookups.add_table(\"lemma_lookup\", lemma_dict)\n",
    "lookups.to_disk(lookups_path)\n",
    "\n",
    "print(f\"‚úÖ Lemma lookup table generated, mode: {files}, saved at: {lookups_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29989503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 5ÔºöTrain RoBERTa large model\n",
    "# ================================\n",
    "!python -m spacy train ./config/config_roberta_large_5.cfg \\\n",
    "    --output ./models/lv_roberta_large \\\n",
    "    --gpu-id 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# Cell 6: Evaluation (parametric)\n",
    "# ========================\n",
    "model_path = f\"./models/lv_roberta_large/model-best\"\n",
    "test_path = \"/home/jesse/Projects/myprojs/Master_Thesis/Fengdi_Huang_Master_Thesis_Repo/Corpus/test/lv_lvtb-ud-test.spacy\"\n",
    "\n",
    "!python -m spacy evaluate {model_path} {test_path} --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d87ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# Cell 7ÔºöAdd Lemmatizer (lookup) to model + Copy LICENSE, LICENSE_SOURCES & README\n",
    "# =================================================================================\n",
    "import spacy\n",
    "from spacy.lookups import Lookups\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "trained_model_path = \"./models/lv_roberta_large/model-best\"\n",
    "final_model_path = Path(\"./models/lv_roberta_large/model_roberta_large\")\n",
    "lookups_path = \"./lookups_lv\"\n",
    "\n",
    "# Load trained model\n",
    "nlp = spacy.load(trained_model_path)\n",
    "\n",
    "# Add lookups\n",
    "lookups = Lookups().from_disk(lookups_path)\n",
    "\n",
    "# Add lemmatizer to pipeline\n",
    "lemmatizer = nlp.add_pipe(\"lemmatizer\", config={\"mode\": \"lookup\"}, last=True)\n",
    "lemmatizer.lookups = lookups  # assign properties directly here\n",
    "\n",
    "# Save new model with lemmatizer components\n",
    "nlp.to_disk(final_model_path)\n",
    "print(f\"‚úÖ Model saved to: {final_model_path} with lemmatizer + lookups\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e816ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Cell 8ÔºöPackaging\n",
    "# =======================\n",
    "\n",
    "from spacy.cli.package import package\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "project_root = Path(\".\").resolve()\n",
    "os.environ[\"PYTHONPATH\"] = f\"{project_root}:{os.environ.get('PYTHONPATH','')}\"\n",
    "\n",
    "# Note that the string path is replaced with a Path object (don't know why but it works only this way)\n",
    "package(\n",
    "    input_dir=Path(final_model_path),\n",
    "    output_dir=Path(package_output_dir),\n",
    "    name=\"roberta_large_5pct\",\n",
    "    version=\"1.0.0\",\n",
    "    force=True\n",
    ")\n",
    "\n",
    "print(f\"Finished, packaged model can be found here: {package_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87665a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Cell 9Ôºö Build wheel + sdist\n",
    "# ===============================\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "package_output_dir = Path(\"./packages/lv_roberta_large_5pct-1.0.0\")\n",
    "dist_dir = package_output_dir / \"dist\"\n",
    "\n",
    "print(f\"‚úÖ sdist is ready. You can install it with pip from: {dist_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f62d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 10B: Install with 'tar.gz'\n",
    "# ================================\n",
    "import subprocess\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Install with '.tar.gz'\n",
    "subprocess.run([\"pip\", \"install\", \"./packages/lv_roberta_large_5pct-1.0.0/dist/lv_roberta_large_5pct-1.0.0.tar.gz\"])\n",
    "nlp_xlmr = spacy.load(\"lv_roberta_large_5pct\")\n",
    "\n",
    "print(\"lv_roberta_large Pipeline components:\", nlp_xlmr.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203adc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================\n",
    "# Cell 11A: Demo Testing\n",
    "# ==================\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Load the pipeline\n",
    "nlp = spacy.load(\"lv_roberta_large_5pct\")\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"Baltijas j≈´ras nosaukums ir devis nosaukumu baltu valodƒÅm un Baltijas valstƒ´m.\n",
    "Terminu \"Baltijas j≈´ra\" (Mare Balticum) pirmoreiz lietoja vƒÅcu hronists Brƒìmenes ƒÄdams 11. gadsimtƒÅ.\"\"\"\n",
    "\n",
    "# Process text\n",
    "doc = nlp(text)\n",
    "\n",
    "# ---------------\n",
    "# Tokenization \n",
    "# ---------------\n",
    "print(\"Tokens: \")\n",
    "print([token.text for token in doc])\n",
    "\n",
    "# ---------------\n",
    "# Lemmatization \n",
    "# ---------------\n",
    "print(\"Lemmas: \")\n",
    "print([token.lemma_ for token in doc])\n",
    "\n",
    "# ------------------------\n",
    "# Part-of-Speech Tagging \n",
    "# ------------------------\n",
    "print(\"POS tags:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.pos_} ({token.tag_})\")\n",
    "\n",
    "# ------------------------\n",
    "# Morphological Features\n",
    "# ------------------------\n",
    "print(\"Morphological features:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.morph}\")\n",
    "\n",
    "# ------------------------\n",
    "# Dependency Parsing \n",
    "# ------------------------\n",
    "print(\"Dependency parsing:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} <--{token.dep_}-- {token.head.text}\")\n",
    "\n",
    "# ------------------------\n",
    "# Sentence Segmentation \n",
    "# ------------------------\n",
    "print(\"Sentences:\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "\n",
    "# ------------------------\n",
    "# Print pipeline components\n",
    "# ------------------------\n",
    "print(\"Pipeline components: \")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Tok2Vec\n",
    "vectors = np.vstack([token.vector for token in doc])\n",
    "print(\"Token vectors shape / Token:\", vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8642c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# Cell 11B: Testing model, espically Lemma and senter\n",
    "# ===================================================\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"lv_roberta_large_5pct\")\n",
    "\n",
    "text = \"\"\"Rƒ´ga ir Latvijas galvaspilsƒìta un viens no galvenajiem r≈´pniecƒ´bas, darƒ´jumu, kult≈´ras, sporta un finan≈°u centriem Baltijas valstƒ´s, kƒÅ arƒ´ nozƒ´mƒ´ga ostas pilsƒìta. Ar 605 273 iedzƒ´votƒÅjiem (2024. gada dati) tƒÅ ir lielƒÅkƒÅ apdzƒ´votƒÅ vieta LatvijƒÅ. TƒÅs robe≈æƒÅs dzƒ´vo aptuveni viena tre≈°daƒºa, bet Rƒ´gas aglomerƒÅcijƒÅ ‚Äî vairƒÅk nekƒÅ puse visu Latvijas iedzƒ´votƒÅju. Pilsƒìtas teritorijas platƒ´ba ir 307,17 km2. Rƒ´gas vƒìsturiskais centrs ir iekƒºauts UNESCO Pasaules kult≈´ras mantojuma sarakstƒÅ un ir ievƒìrojams ar j≈´gendstila arhitekt≈´ru, kurai, pƒìc UNESCO viedokƒºa, nav lƒ´dzƒ´gu pasaulƒì. Kop≈° dibinƒÅ≈°anas 1201. gadƒÅ lƒ´dz m≈´su dienƒÅm Rƒ´ga ir Baltijas valstu lielƒÅkƒÅ pilsƒìta un viena no ievƒìrojamƒÅkajƒÅm ostƒÅm Baltijas j≈´ras austrumdaƒºƒÅ. Politiski un administratƒ´vi tƒÅ ilgu laiku bijusi reƒ£iona politiskais centrs, bet sƒÅkot ar 20. gadsimtu ‚Äî Latvijas Republikas galvaspilsƒìta.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Generate Token Table\n",
    "rows = []\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        rows.append({\n",
    "            \"Text\": token.text,\n",
    "            \"Lemma\": token.lemma_,\n",
    "            \"POS\": token.pos_,\n",
    "            \"Dependency\": token.dep_,\n",
    "            \"Head\": token.head.text\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "display(df) \n",
    "\n",
    "# Sentence Segmentation\n",
    "print(\"\\nSentence Segmentation results:\")\n",
    "for i, sent in enumerate(doc.sents, 1):\n",
    "    print(f\"Sentence {i}: {sent.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3839b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# === 1. Set file paths ===\n",
    "train_path = \"/home/jesse/Projects/myprojs/Master_Thesis/Fengdi_Huang_Master_Thesis_Repo/Corpus/train/lv_lvtb-ud-train-5pct.spacy\"\n",
    "test_path  = \"/home/jesse/Projects/myprojs/Master_Thesis/Fengdi_Huang_Master_Thesis_Repo/Corpus/test/lv_lvtb-ud-test.spacy\"\n",
    "\n",
    "# === 2. Load the model ===\n",
    "nlp = spacy.load(\"lv_roberta_large_5pct\")\n",
    "\n",
    "# === 3. Load the training data and build the vocabulary ===\n",
    "train_vocab = set()\n",
    "train_docs = DocBin().from_disk(train_path)\n",
    "for doc in train_docs.get_docs(nlp.vocab):\n",
    "    for token in doc:\n",
    "        train_vocab.add(token.text)\n",
    "\n",
    "print(f\"‚úÖ Training vocabulary size: {len(train_vocab)}\")\n",
    "\n",
    "# === 4. Load the test data ===\n",
    "test_docs = DocBin().from_disk(test_path)\n",
    "test_docs = list(test_docs.get_docs(nlp.vocab))\n",
    "print(f\"‚úÖ Number of test documents: {len(test_docs)}\")\n",
    "\n",
    "# === 5. Initialize counters for all components ===\n",
    "metrics = {\n",
    "    \"POS\": {\"iv_total\": 0, \"iv_correct\": 0, \"oov_total\": 0, \"oov_correct\": 0},\n",
    "    \"MORPH\": {\"iv_total\": 0, \"iv_correct\": 0, \"oov_total\": 0, \"oov_correct\": 0},\n",
    "    \"LEMMA\": {\"iv_total\": 0, \"iv_correct\": 0, \"oov_total\": 0, \"oov_correct\": 0},\n",
    "    \"UAS\": {\"iv_total\": 0, \"iv_correct\": 0, \"oov_total\": 0, \"oov_correct\": 0},\n",
    "    \"LAS\": {\"iv_total\": 0, \"iv_correct\": 0, \"oov_total\": 0, \"oov_correct\": 0},\n",
    "}\n",
    "\n",
    "# === 6. Run predictions and calculate metrics ===\n",
    "for gold_doc in test_docs:\n",
    "    pred_doc = nlp(gold_doc.text)\n",
    "    for gold_token, pred_token in zip(gold_doc, pred_doc):\n",
    "        is_iv = gold_token.text in train_vocab\n",
    "\n",
    "        # POS accuracy\n",
    "        if is_iv:\n",
    "            metrics[\"POS\"][\"iv_total\"] += 1\n",
    "            if gold_token.pos_ == pred_token.pos_:\n",
    "                metrics[\"POS\"][\"iv_correct\"] += 1\n",
    "        else:\n",
    "            metrics[\"POS\"][\"oov_total\"] += 1\n",
    "            if gold_token.pos_ == pred_token.pos_:\n",
    "                metrics[\"POS\"][\"oov_correct\"] += 1\n",
    "\n",
    "        # Morphology accuracy (exact match)\n",
    "        gold_morph = gold_token.morph.to_dict()\n",
    "        pred_morph = pred_token.morph.to_dict()\n",
    "        if is_iv:\n",
    "            metrics[\"MORPH\"][\"iv_total\"] += 1\n",
    "            if gold_morph == pred_morph:\n",
    "                metrics[\"MORPH\"][\"iv_correct\"] += 1\n",
    "        else:\n",
    "            metrics[\"MORPH\"][\"oov_total\"] += 1\n",
    "            if gold_morph == pred_morph:\n",
    "                metrics[\"MORPH\"][\"oov_correct\"] += 1\n",
    "\n",
    "        # Lemmatization accuracy\n",
    "        if is_iv:\n",
    "            metrics[\"LEMMA\"][\"iv_total\"] += 1\n",
    "            if gold_token.lemma_ == pred_token.lemma_:\n",
    "                metrics[\"LEMMA\"][\"iv_correct\"] += 1\n",
    "        else:\n",
    "            metrics[\"LEMMA\"][\"oov_total\"] += 1\n",
    "            if gold_token.lemma_ == pred_token.lemma_:\n",
    "                metrics[\"LEMMA\"][\"oov_correct\"] += 1\n",
    "\n",
    "        # Parsing: UAS / LAS\n",
    "        if gold_token.head is not None:\n",
    "            if is_iv:\n",
    "                metrics[\"UAS\"][\"iv_total\"] += 1\n",
    "                metrics[\"LAS\"][\"iv_total\"] += 1\n",
    "                if gold_token.head.i == pred_token.head.i:\n",
    "                    metrics[\"UAS\"][\"iv_correct\"] += 1\n",
    "                    if gold_token.dep_ == pred_token.dep_:\n",
    "                        metrics[\"LAS\"][\"iv_correct\"] += 1\n",
    "            else:\n",
    "                metrics[\"UAS\"][\"oov_total\"] += 1\n",
    "                metrics[\"LAS\"][\"oov_total\"] += 1\n",
    "                if gold_token.head.i == pred_token.head.i:\n",
    "                    metrics[\"UAS\"][\"oov_correct\"] += 1\n",
    "                    if gold_token.dep_ == pred_token.dep_:\n",
    "                        metrics[\"LAS\"][\"oov_correct\"] += 1\n",
    "\n",
    "# === 7. Print the results ===\n",
    "print(\"\\nüéØ OOV/IV Accuracy Results\")\n",
    "print(\"--------------------------------------------------\")\n",
    "for comp, m in metrics.items():\n",
    "    iv_acc = m[\"iv_correct\"] / m[\"iv_total\"] * 100 if m[\"iv_total\"] > 0 else 0\n",
    "    oov_acc = m[\"oov_correct\"] / m[\"oov_total\"] * 100 if m[\"oov_total\"] > 0 else 0\n",
    "    print(f\"{comp}:\")\n",
    "    print(f\"  IV  Accuracy  = {iv_acc:.2f}%  ({m['iv_correct']}/{m['iv_total']})\")\n",
    "    print(f\"  OOV Accuracy = {oov_acc:.2f}%  ({m['oov_correct']}/{m['oov_total']})\")\n",
    "    print(\"--------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_lv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
