{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1733d69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (1506 documents):\n",
      "train/lv_lvtb-ud-train.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (211 documents): train/lv_lvtb-ud-dev.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (242 documents):\n",
      "train/lv_lvtb-ud-test.spacy\u001b[0m\n",
      "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
      "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
      "into documents with `-n 10`.\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (2412 documents):\n",
      "test/lv_lvtb-ud-test.spacy\u001b[0m\n",
      "All conllu files are converted to spaCy Format.\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# Cell 1：Convert conllu to spaCy format\n",
    "# ======================================\n",
    "!python -m spacy convert /home/jesse/Projects/myprojs/Master_Thesis/Fengdi_Huang_Master_Thesis_Repo/UD_Latvian_LVTB/lv_lvtb-ud-train.conllu ./train -n 10 \n",
    "!python -m spacy convert /home/jesse/Projects/myprojs/Master_Thesis/Fengdi_Huang_Master_Thesis_Repo/UD_Latvian_LVTB/lv_lvtb-ud-dev.conllu ./train -n 10 \n",
    "!python -m spacy convert /home/jesse/Projects/myprojs/Master_Thesis/Fengdi_Huang_Master_Thesis_Repo/UD_Latvian_LVTB/lv_lvtb-ud-test.conllu ./train -n 10\n",
    "\n",
    "# For testing\n",
    "!python -m spacy convert /home/jesse/Projects/myprojs/Master_Thesis/Fengdi_Huang_Master_Thesis_Repo/UD_Latvian_LVTB/lv_lvtb-ud-test.conllu ./test\n",
    "\"\"\"\n",
    "Using a lookup table for lemmatization matches words solely based on their surface form (or lowercase),\n",
    "without considering context. In longer documents (multiple sentences or complex structures):\n",
    "\n",
    "    - spaCy's lemmatization may be indirectly affected by pipeline processing and Vocab caching. \n",
    "      For example, repeated tokens or subtle variations in capitalization/punctuation can lead \n",
    "      to lookup misses.\n",
    "    - Some compound or modified words might not exist in the lookup table.\n",
    "\n",
    "As a result, longer documents increase the likelihood of lookup failures, reducing overall lemma accuracy.\n",
    "\n",
    "To balance this, during training we group 10 sentences per Doc to provide richer context for\n",
    "sentence segmentation learning. For evaluating lemma performance, however, we use a test set\n",
    "with one sentence per Doc, which isolates lemma accuracy from potential inter-sentence effects.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"All conllu files are converted to spaCy Format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23cfefa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved first 50% of training data to: /home/jesse/Projects/myprojs/Master_Thesis/Fengdi_Huang_Master_Thesis_Repo/Corpus/train/lv_lvtb-ud-train-50pct.spacy\n",
      "Original samples: 1506 | New subset: 753\n",
      "✅ Saved first 20% of training data to: /home/jesse/Projects/myprojs/Master_Thesis/Fengdi_Huang_Master_Thesis_Repo/Corpus/train/lv_lvtb-ud-train-20pct.spacy\n",
      "Original samples: 1506 | New subset: 301\n",
      "✅ Saved first 5% of training data to: /home/jesse/Projects/myprojs/Master_Thesis/Fengdi_Huang_Master_Thesis_Repo/Corpus/train/lv_lvtb-ud-train-5pct.spacy\n",
      "Original samples: 1506 | New subset: 75\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# Cell 2: Split training data (e.g., 50%, 20%)\n",
    "# ===============================================\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from pathlib import Path\n",
    "\n",
    "# corpus dir\n",
    "corpus_train_path = Path(\"/home/jesse/Projects/myprojs/Master_Thesis/Fengdi_Huang_Master_Thesis_Repo/Corpus/train\")\n",
    "\n",
    "# Dir\n",
    "train_path = corpus_train_path / \"lv_lvtb-ud-train.spacy\"\n",
    "\n",
    "train_100_path = corpus_train_path / \"lv_lvtb-ud-train-100pct.spacy\"\n",
    "train_50_path = corpus_train_path / \"lv_lvtb-ud-train-50pct.spacy\"\n",
    "train_20_path = corpus_train_path / \"lv_lvtb-ud-train-20pct.spacy\"\n",
    "train_5_path = corpus_train_path / \"lv_lvtb-ud-train-5pct.spacy\"\n",
    "\n",
    "# Load corpora\n",
    "nlp_blank = spacy.blank(\"lv\") \n",
    "docbin = DocBin().from_disk(train_path)\n",
    "docs = list(docbin.get_docs(nlp_blank.vocab))\n",
    "\n",
    "# keep 100%\n",
    "docs_100pct = docs\n",
    "\n",
    "#save .spacy file\n",
    "docbin_100 = DocBin(docs=docs_100pct)\n",
    "docbin_100.to_disk(train_100_path)\n",
    "\n",
    "# keep top 50%\n",
    "half_len = len(docs) // 2\n",
    "docs_50pct = docs[:half_len]\n",
    "\n",
    "# save .spacy file\n",
    "docbin_50 = DocBin(docs=docs_50pct)\n",
    "docbin_50.to_disk(train_50_path)\n",
    "\n",
    "# keep top 20%\n",
    "fifth_len = len(docs) // 5\n",
    "docs_20pct = docs[:fifth_len]\n",
    "\n",
    "# save .spacy file\n",
    "docbin_20 = DocBin(docs=docs_20pct)\n",
    "docbin_20.to_disk(train_20_path)\n",
    "\n",
    "# keep top 5%\n",
    "fifth_len = len(docs) // 20\n",
    "docs_5pct = docs[:fifth_len]\n",
    "\n",
    "# save .spacy file\n",
    "docbin_5 = DocBin(docs=docs_5pct)\n",
    "docbin_5.to_disk(train_5_path)\n",
    "\n",
    "print(f\"✅ Saved first 50% of training data to: {train_50_path}\")\n",
    "print(f\"Original samples: {len(docs)} | New subset: {len(docs_50pct)}\")\n",
    "\n",
    "print(f\"✅ Saved first 20% of training data to: {train_20_path}\")\n",
    "print(f\"Original samples: {len(docs)} | New subset: {len(docs_20pct)}\")\n",
    "\n",
    "print(f\"✅ Saved first 5% of training data to: {train_5_path}\")\n",
    "print(f\"Original samples: {len(docs)} | New subset: {len(docs_5pct)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
